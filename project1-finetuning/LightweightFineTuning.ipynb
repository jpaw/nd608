{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e136727-0c48-4a95-bf5f-8ee213327b4f",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project\n",
    "\n",
    "## Table of contents\n",
    "0. [Decisions](#config)\n",
    "1. [Loading and Evaluating a Foundation Model](#loadbase)\n",
    "2. [Performing Parameter-Efficient Fine-Tuning](#peft)\n",
    "3. [Performing Inference with a PEFT Model](#inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a8b905-49c8-4aaa-a4da-d95f0d1908b6",
   "metadata": {},
   "source": [
    "## 0. Decisions\n",
    "<a id=\"config\"></a>\n",
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: As recommended, I will use LoRA \n",
    "* Model: I learned that gpt is better for CausalLM, therefore I will use bert instead of gpt-2\n",
    "* Evaluation approach: TODO\n",
    "* Fine-tuning dataset: Despite being similar to imdb, I will use movie reviews, but here from rotten tomatos. Main reason is it is the first data set I find which has an appropriate size (5331 records per sentiment, with a 50/50 distribution). Of that, we only use 33.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a7e9d-8d4f-42b1-a5f2-2687e42e67d6",
   "metadata": {},
   "source": [
    "## 1. Loading and Evaluating a Foundation Model\n",
    "<a id=\"loadbase\"></a>\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers as tf  # probably a bad idea, tf could be mistaken for tensorflow, will pick another abbrev next time\n",
    "import datasets as ds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate as eva\n",
    "import torch.nn.functional as func\n",
    "import peft\n",
    "\n",
    "# config: pick one of the next 3\n",
    "# THE_MODEL_I_USE = \"gpt2\"                  # suggested, but not that suitable for classification\n",
    "THE_MODEL_I_USE = \"distilbert-base-uncased\" # current selection\n",
    "\n",
    "# config: select a metric\n",
    "# THE_METRIC_I_USE = \"accuracy\"\n",
    "THE_METRIC_I_USE = \"f1\"                     # should be better for imbalanced data sets\n",
    "\n",
    "# Config: train the classiication head? If not, the initial evaluation is more or less random, if done, these weights won't be saved and are lost\n",
    "# According to guidelines, it should NOT be done\n",
    "TRAIN_CH = False\n",
    "\n",
    "# the name of the tuned model is the original one plus some suffix\n",
    "NAME_OF_FINETUNED_MODEL = THE_MODEL_I_USE + \"-loratuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count t/v/t is 2844/356/356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 356\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data set\n",
    "allsets = ds.load_dataset(\"rotten_tomatoes\")\n",
    "# to speed things up, we want to use 1 third of the data set only\n",
    "only_a_third = lambda data, index: index % 3 == 0\n",
    "\n",
    "# split into its parts\n",
    "part_train      = allsets[\"train\"]     .filter(only_a_third, with_indices=True)\n",
    "part_validation = allsets[\"validation\"].filter(only_a_third, with_indices=True)\n",
    "part_test       = allsets[\"test\"]      .filter(only_a_third, with_indices=True)\n",
    "\n",
    "# print sizes\n",
    "print(f\"Count t/v/t is {len(part_train)}/{len(part_validation)}/{len(part_test)}\")\n",
    "# => awesome, some decent size!\n",
    "\n",
    "# inspect it:\n",
    "part_test\n",
    "# => has text and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "I obtained Bleh! for This was a really bad movie\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf573716cbf048aebffb2db0ecfb3c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2844 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a3830bad3d49cfb0c9daf9c7cb8749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/356 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff277d3d5ec4fcea8376bb058537355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/356 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def to_label(token):\n",
    "    return \"Bleh!\" if token == 0 else \"Yeah!\"\n",
    "\n",
    "original_tokenizer = tf.AutoTokenizer.from_pretrained(THE_MODEL_I_USE) # , return_tensors=\"pt\")\n",
    "original_model = tf.AutoModelForSequenceClassification.from_pretrained(THE_MODEL_I_USE, num_labels=2,\n",
    "    id2label={0: \"Bleh!\", 1: \"Yeah!\"},\n",
    "    label2id={\"Bleh!\": 0, \"Yeah!\": 1}\n",
    ")\n",
    "\n",
    "# set padding (quite an effort to make it work!)\n",
    "original_tokenizer.pad_token = original_tokenizer.eos_token\n",
    "original_model.config.pad_token_id = original_model.config.eos_token_id\n",
    "original_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "print(original_model)\n",
    "\n",
    "# test the tokenizer on some fixed text\n",
    "sample_text = \"This was a really bad movie\"\n",
    "sample_inputs = original_tokenizer(sample_text, return_tensors=\"pt\")\n",
    "sample_inputs[\"input_ids\"]\n",
    "sample_result = original_model.forward(sample_inputs[\"input_ids\"])\n",
    "sample_result_label = to_label(np.argmax(sample_result.logits.detach, -1))\n",
    "# print(f'I obtained {sample_inputs}, {np.argmax(sample_result.logits.detach, -1)}')\n",
    "print(f'I obtained {sample_result_label} for {sample_text}')\n",
    "\n",
    "# tokenize the stuff\n",
    "def tokenize(dsin):\n",
    "    return dsin.map(lambda x: original_tokenizer(x[\"text\"], padding=\"max_length\", truncation=True), batched=True)\n",
    "\n",
    "tok_train      = tokenize(part_train)\n",
    "tok_validation = tokenize(part_validation)\n",
    "tok_test       = tokenize(part_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93424b86632440d38b88a166c5c09913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I obtained Bleh! for lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\n",
      "I obtained Bleh! for the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .\n",
      "I obtained Bleh! for throws in enough clever and unexpected twists to make the formula feel fresh .\n",
      "I obtained Bleh! for generates an enormous feeling of empathy for its characters .\n",
      "I obtained Bleh! for mostly , [goldbacher] just lets her complicated characters be unruly , confusing and , through it all , human .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# standard metric used for classification (I've seen f1 as well...)\n",
    "#def compute_metrics(eval_pred):\n",
    "#    predictions, labels = eval_pred\n",
    "#    predictions = np.argmax(predictions, axis=-1)\n",
    "#    return { \"accuracy\": (predictions == labels).mean() } \n",
    "\n",
    "metric = eva.load(THE_METRIC_I_USE)\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# determine how good the untrained original model is, by prompting it for the answers\n",
    "for i in range(5):\n",
    "    review_to_check = part_test[\"text\"][i]\n",
    "    # prompt = 'Do you think the following review is positive? ' + review_to_check # construct a prompt\n",
    "    prompt = review_to_check\n",
    "    tokens = original_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    # print(f'Type of tokens is {type(tokens)} and tokens are {tokens}')\n",
    "    answer = original_model(**tokens)\n",
    "    logits = answer.logits\n",
    "    result_label = to_label(np.argmax(logits.detach, -1))\n",
    "    # print(logits)\n",
    "    print(f'I obtained {result_label} for {review_to_check}')\n",
    "    # probabilities = func.softmax(logits[0], dim=-1)  # argmax?\n",
    "    #print(f'The guess for {review_to_check} is')\n",
    "\n",
    "# That does not yet look meaningful at all! :-("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mbi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "===========================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='89' max='89' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [89/89 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7017835974693298,\n",
       " 'eval_f1': 0.6666666666666666,\n",
       " 'eval_runtime': 1.8136,\n",
       " 'eval_samples_per_second': 196.294,\n",
       " 'eval_steps_per_second': 49.073}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attach a classification head!\n",
    "\n",
    "# Freeze all the parameters of the base model\n",
    "for param in original_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "original_model.classifier\n",
    "\n",
    "# possible trainer directly on base model (training not executed)\n",
    "trainer_of_ch = tf.Trainer(\n",
    "    model=original_model,\n",
    "    args=tf.TrainingArguments(\n",
    "        output_dir=\"./data/ch\",\n",
    "        learning_rate=2e-3,\n",
    "        # Reduce the batch size if you don't have enough memory\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset = tok_train,\n",
    "    eval_dataset  = tok_validation,\n",
    "    tokenizer     = original_tokenizer,\n",
    "    data_collator = tf.DataCollatorWithPadding(tokenizer=original_tokenizer),\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n",
    "print('===========================================')\n",
    "if TRAIN_CH:\n",
    "    trainer_of_ch.train()\n",
    "print('===========================================')\n",
    "\n",
    "trainer_of_ch.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## 2. Performing Parameter-Efficient Fine-Tuning\n",
    "<a id=\"peft\"></a>\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 665,858 || all params: 67,620,868 || trainable%: 0.9846930684178736\n",
      "Now the work starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mbi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2844' max='2844' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2844/2844 02:22, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.618000</td>\n",
       "      <td>0.406783</td>\n",
       "      <td>0.816901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.458100</td>\n",
       "      <td>0.475466</td>\n",
       "      <td>0.826590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.332900</td>\n",
       "      <td>0.641566</td>\n",
       "      <td>0.824859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.199900</td>\n",
       "      <td>0.796972</td>\n",
       "      <td>0.810345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a LoRA fine tuning\n",
    "\n",
    "# config for \"bert-base-cased\"\n",
    "lora_config = peft.LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    r=4,           # wild guess, LoRA paper used values between 1 and 16...\n",
    "    lora_alpha=4,  # same as r?\n",
    "    lora_dropout=0.01,\n",
    ")\n",
    "if THE_MODEL_I_USE == \"distilbert-base-uncased\":\n",
    "    lora_config = peft.LoraConfig(\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        r=4,\n",
    "        lora_alpha=4,\n",
    "        lora_dropout=0.01,\n",
    "        # target_modules=[\"q_lin\", \"v_lin\", \"k_lin\", \"out_lin\"],  # this is the theory but stays at 0.5 accuracy\n",
    "        target_modules=[\"q_lin\", \"v_lin\"],  # this is the theory but stays at 0.5 accuracy\n",
    "        # target_modules=[\"pre_classifier\"],  # throws exception despite also being of type Linear\n",
    "        # target_modules=[\"classifier\"],  # achieves 0.82\n",
    "    )\n",
    "\n",
    "\n",
    "# lora_model = peft.LoraModel(original_model, lora_config, \"default\")\n",
    "lora_model = peft.get_peft_model(original_model, lora_config)\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "original_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "trainer_for_lora = tf.Trainer(\n",
    "    model = lora_model,\n",
    "    args  = tf.TrainingArguments(\n",
    "        output_dir                  = \"./train/\", # weights saved to some subfolder\n",
    "        learning_rate               = 0.002,\n",
    "        per_device_train_batch_size = 4,\n",
    "        per_device_eval_batch_size  = 4,\n",
    "        evaluation_strategy         = \"epoch\",\n",
    "        save_strategy               = \"epoch\",\n",
    "        num_train_epochs            = 4,     # with GPU 4 epochs take 3 minutes, without... hours! but no improvement after epoch 2\n",
    "        weight_decay                = 0.01,\n",
    "        load_best_model_at_end      = True,\n",
    "    ),\n",
    "    train_dataset   = tok_train,\n",
    "    eval_dataset    = tok_validation,\n",
    "    tokenizer       = original_tokenizer,\n",
    "    data_collator   = tf.DataCollatorWithPadding(tokenizer=original_tokenizer),\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Now the work starts\")\n",
    "trainer_for_lora.train()\n",
    "trainer_for_lora.evaluate()\n",
    "lora_model.save_pretrained(NAME_OF_FINETUNED_MODEL)  # use current directory as required in rubric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## 3. Performing Inference with a PEFT Model\n",
    "<a id=\"inference\"></a>\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): DistilBertForSequenceClassification(\n",
      "      (distilbert): DistilBertModel(\n",
      "        (embeddings): Embeddings(\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (transformer): Transformer(\n",
      "          (layer): ModuleList(\n",
      "            (0-5): 6 x TransformerBlock(\n",
      "              (attention): MultiHeadSelfAttention(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (q_lin): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.01, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=4, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=4, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (v_lin): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.01, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=4, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=4, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (ffn): FFN(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (activation): GELUActivation()\n",
      "              )\n",
      "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pre_classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lora_reloaded = peft.AutoPeftModelForSequenceClassification.from_pretrained(NAME_OF_FINETUNED_MODEL)\n",
    "print(lora_reloaded)\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review is lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\n",
      "Yay!\n",
      "review is the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .\n",
      "Yay!\n",
      "review is throws in enough clever and unexpected twists to make the formula feel fresh .\n",
      "Yay!\n",
      "review is generates an enormous feeling of empathy for its characters .\n",
      "Yay!\n",
      "review is mostly , [goldbacher] just lets her complicated characters be unruly , confusing and , through it all , human .\n",
      "Nay!\n"
     ]
    }
   ],
   "source": [
    "# just for visual checks\n",
    "for i in range(5):\n",
    "    review_to_check = part_test[\"text\"][i]\n",
    "    print(f'review is {review_to_check}')\n",
    "    tokens_of_review = original_tokenizer(review_to_check, return_tensors=\"pt\")\n",
    "    # print(f'tokens are {tokens_of_review.input_ids}')\n",
    "    answer_of_finetuned_model = lora_reloaded(input_ids=tokens_of_review.input_ids)\n",
    "    # print(f'answer is {answer_of_finetuned_model}')\n",
    "    logits = answer_of_finetuned_model.logits\n",
    "    probs = logits[0]\n",
    "    result =  \"Yay!\" if probs[0] < probs[1] else \"Nay!\"\n",
    "    print(result)\n",
    "    # print(type(logits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 0.4298180043697357, 'test_f1': 0.8126801152737753, 'test_runtime': 2.011, 'test_samples_per_second': 177.029, 'test_steps_per_second': 44.257}\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(tok_test)\n",
    "df = df[[\"text\", \"label\"]]\n",
    "\n",
    "# Add the model predictions to the dataframe\n",
    "# Why is predict part of the trainer?\n",
    "predictions = trainer_for_lora.predict(tok_test)\n",
    "\n",
    "print(predictions.metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
